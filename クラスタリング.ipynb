{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyN1w0tvBdqrHSPlOj9dgOmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachotta3227/python/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新しいセクション"
      ],
      "metadata": {
        "id": "Uc9wPkyLX6mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(5)\n",
        "dataser=pd.read_csv('retail_dataset.csv',\n",
        "                    seq=',',\n",
        "                    header=0,\n",
        "                    index_col=None)\n"
      ],
      "metadata": {
        "id": "GPRsAxACW3YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPUNzHU-NlIw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.unicode.east_asian_width', True) #出力を全角文字幅で表示する\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ１： データセットを用意する\n",
        "# あるスーパーマーケットのレシートデータベースの一部を想定する。レシートのことをトランザクションと呼ぶ。\n",
        "# データセットは、下記の通りとなり、一行をレシート一枚と対応する。\n",
        "#---------------------------------------------------------------------------------------------\n",
        "dataset = [['ミルク', '玉葱', 'ナツメグ', 'インゲンマメ', 'たまご', 'ヨーグルト'],\n",
        "           ['イノンド', '玉葱', 'ナツメグ', 'インゲンマメ', 'たまご', 'ヨーグルト'],\n",
        "           ['ミルク', 'リンゴ', 'インゲンマメ', 'たまご'],\n",
        "           ['ミルク', 'ユニコーン', 'コーン', 'インゲンマメ', 'ヨーグルト'],\n",
        "           ['コーン', '玉葱', '玉葱', 'インゲンマメ', 'アイスクリーム', 'たまご']]\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"========== あるスーパーマーケットのレシートデータベースの一部 ==========\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "for i in range(0,len(dataset)):\n",
        "    print(dataset[i])\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ２：  データの前処理\n",
        "# MlxtendライブラリーのAprioriモジュールを利用するために、レシートを0/1 または True/Falseで表す必要がある。\n",
        "# その商品の購入があった場合は１で表し、なかった場合は０で表す。\n",
        "#       商品1 商品2 商品3 ... 商品N\n",
        "#顧客1    1    0     0        1\n",
        "#顧客2    0    1     1        0\n",
        "#....\n",
        "#顧客M    1    1     0        1\n",
        "#---------------------------------------------------------------------------------------------\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(dataset) #変換を実施する\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_) #変換の結果をデータフレームに変換する。\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ３： 設定された支持度（0.6）以上のアイテムセットを頻出アイテムセットとして抽出する\n",
        "#　相関関係マイニングアルゴリズムは３種類あり、apriori, fpmax, fpgrowthとなる。どちらを利用しても良い。\n",
        "#---------------------------------------------------------------------------------------------\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "frequent_itemsets   = apriori(df, min_support=0.6, use_colnames=True)  #1. apriori法\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"====================== 頻出アイテムセットと支持度 ======================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(frequent_itemsets)\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ４.1： 設定された信頼度（0.7）以上の相関ルールを全て抽出する\n",
        "#---------------------------------------------------------------------------------------------\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"========================== 相関ルールと信頼度 ==========================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(rules)\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ4.2： 与えられた条件部を満たす相関ルール（の帰結部）を抽出する\n",
        "#---------------------------------------------------------------------------------------------\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"================ ある与えられた条件部を満たす相関ルール ================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(rules[rules['antecedents'] == {'たまご', 'インゲンマメ'}])\n",
        "print(\"------------------------\\n\")\n",
        "###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.unicode.east_asian_width', True) #出力を全角文字幅で表示する\n",
        "#1\n",
        "dataset=[['天気良好','酒酔い','スピード違反','シートベルト未着用','大きな衝突'],\n",
        "           ['天気荒れ','酔ってない','交通違反なし','シートベルト着用','小さな衝突'],\n",
        "           ['天気良好','酔ってない','一時停止違反','シートベルト着用','小さな衝突'],\n",
        "           ['天気良好','酔ってない','スピード違反','シートベルト着用','大きな衝突'],\n",
        "           ['天気荒れ','酔ってない','一時停止違反','シートベルト未着用','大きな衝突'],\n",
        "           ['天気良好','酒酔い','一時停止違反','シートベルト着用','小さな衝突'],\n",
        "           ['天気荒れ','酒酔い','交通違反なし','シートベルト着用','大きな衝突'],\n",
        "           ['天気良好','酔ってない','一時停止違反','シートベルト着用','大きな衝突'],\n",
        "           ['天気良好','酒酔い','交通違反なし','シートベルト未着用','大きな衝突'],\n",
        "           ['天気荒れ','酔ってない','一時停止違反','シートベルト未着用','大きな衝突'],\n",
        "           ['天気良好','酒酔い','スピード違反','シートベルト着用','大きな衝突'],\n",
        "           ['天気荒れ','酔ってない','一時停止違反','シートベルト着用','小さな衝突']]\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"========== あるスーパーマーケットのレシートデータベースの一部 ==========\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "for i in range(0,len(dataset)):\n",
        "    print(dataset[i])\n",
        "print(\"------------------------\\n\")\n",
        "#2\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit_transform(dataset) #変換を実施する\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "te.columns_\n",
        "\n",
        "#3\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "frequent_itemsets   = apriori(df, min_support=0.25, use_colnames=True)  #1. apriori法\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"====================== 頻出アイテムセットと支持度 ======================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(frequent_itemsets)\n",
        "print(\"------------------------\\n\")\n",
        "#4\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"========================== 相関ルールと信頼度 ==========================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(rules)\n",
        "print(\"------------------------\\n\")\n",
        "#4-2\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(\"================ ある与えられた条件部を満たす相関ルール ================\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "print(rules[rules['antecedents'] == {'たまご', 'インゲンマメ'}])\n",
        "print(\"------------------------\\n\")\n",
        "###"
      ],
      "metadata": {
        "id": "6q0IEyGqQdWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.unicode.east_asian_width', True)\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "#ステップ１： データセットを用意して\n",
        "#　Pandasオブジェクトとしてのデータフレーム subjectratings に保存する。\n",
        "#--------------------------------------------------------------------\n",
        "subjects = ['学生','数学','物理','国語','歴史']\n",
        "ratings  = [['田中',5,5,2,1],\n",
        "            ['鈴木',4,5,3,2],\n",
        "            ['葉山',2,2,4,5],\n",
        "            ['浦邉',1,2,3,4],\n",
        "            ['村田',4,4,4,3],\n",
        "            ['森田',2,1,5,5],\n",
        "            ['伸太',2,2,2,2]]\n",
        "subjectratings  = pd.DataFrame(ratings,columns=subjects)\n",
        "print(\"ステップ1\")\n",
        "print(\"-------------------------------\")\n",
        "print(subjectratings) #中身を確認する\n",
        "print(\"-------------------------------\\n\")\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "#ステップ２： k-means法でクラスタリングを行う\n",
        "#　１　KMeansを利用するために、列「学生」を削除する必要がある。\n",
        "#　　　axis=1の「1」が列名の行を指す：df.drop([C1,C2...], axis=1)\n",
        "#　２　KMeansのパラメーター\n",
        "#      n_clusters\tクラスタ数（デフォルト値: 8）\n",
        "#--------------------------------------------------------------------\n",
        "from sklearn.cluster import KMeans\n",
        "data = subjectratings.drop('学生',axis=1)\n",
        "k_means = KMeans(n_clusters=2)\n",
        "k_means.fit(data) #クラスタリングの計算を実行する\n",
        "labels = k_means.labels_ \t#分類先となったラベルを取得する\n",
        "outputresult = pd.DataFrame(labels, index=subjectratings.学生, columns=['クラスターID'])\n",
        "print(\"------------------\")\n",
        "print(outputresult)\n",
        "print(\"------------------\\n\")\n",
        "\n",
        "#クラスタリング後の各クラスタのセンターを表示する\n",
        "centroids = k_means.cluster_centers_ #各クラスターのセントロイドの座標を取得する\n",
        "outputresult = pd.DataFrame(centroids,columns=data.columns)\n",
        "print(\"-------------------------------------\")\n",
        "print(outputresult)\n",
        "print(\"-------------------------------------\\n\")\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "#ステップ３： ステップ２で得たモデル k_means を適用し、新しいデータの振分けをする\n",
        "# 　１ KMeansのpredict(X)関数は、Xのサンプルが属しているクラスタ番号を返す。\n",
        "# 　２ reshape： 配列の形を変えるメソッド。例えば、\n",
        "list1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "list2 = list1.reshape(2, 4)\n",
        "list2:  [[1, 2, 3, 4],[5, 6, 7, 8]]\n",
        "#     １次元配列を列ベクトルに変換： １次元配列をreshape(-1, 1)とすると、\n",
        "#     その配列を要素とする２次元１列の配列となる。\n",
        "#--------------------------------------------------------------------\n",
        "import numpy as np\n",
        "experData = np.array([[4,5,1,2],[3,2,4,4],[2,3,4,1],[3,2,3,3],[5,4,1,4]])\n",
        "labels = k_means.predict(experData)\n",
        "labels = labels.reshape(-1,1)\n",
        "usernames = np.array(['福田','成田','赤瀬','森山','福山']).reshape(-1,1)\n",
        "cols = subjectratings.columns.tolist()\n",
        "cols.append('クラスターID')\n",
        "experusers = pd.DataFrame(np.concatenate((usernames, experData, labels), axis=1),columns=cols)\n",
        "print(\"----------------------------------------\")\n",
        "print(\"experusers\")\n",
        "print(experusers)\n",
        "print(\"----------------------------------------\\n\")\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "#ステップ４： k-means法の最適なクラスター数を推定する\n",
        "#--------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "#import japanize_matplotlib\n",
        "\n",
        "numClusters = [1,2,3,4,5]\n",
        "SSE = []\n",
        "for k in numClusters:\n",
        "    k_means = KMeans(n_clusters=k)\n",
        "    k_means.fit(data)\n",
        "    SSE.append(k_means.inertia_)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(numClusters, SSE)\n",
        "plt.xlabel('クラスター数')\n",
        "plt.ylabel('SSE (Sum of Squared Errors)')\n",
        "plt.title('Kの最適値を決めるエルボー法')\n",
        "plt.savefig(\"results.png\")\n",
        "#plt.show()\n",
        "\n",
        "print(\"----- 終了 -----\")\n",
        "#---------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Ufeiby_0TUKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "課題3-2\n"
      ],
      "metadata": {
        "id": "7KUIBKP5XnKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.unicode.east_asian_width', True) #出力を全角文字幅で表示する\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ１： データセットを用意する\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#1. データセットの列の列名を定義する\n",
        "columnNames=['旅行者ID', '教会', 'リゾート', '海岸', '公園', '劇場', '博物館', 'モール', '動物園', 'レストラン', 'パブ/バー', 'ローカルサービス', 'バーガー/ビザ店', 'ホテル施設', 'ジュースバー', '美術館', 'ダンスクラブ', '水泳プール', 'ジム', 'パン屋', 'スパ&ビューティー', 'カフェテラス', '観光名所', '記念物', 'ガーデン']\n",
        "\n",
        "#2. データ解析に実に利用される列の列名を定義する。この解析では、他の列を使わない。\n",
        "targetColumns =['リゾート', '海岸', '公園', '劇場', '博物館', 'レストラン', 'ジュースバー', '美術館', 'ダンスクラブ']\n",
        "\n",
        "#3. CSVファイルをPandasオブジェクトとしてのデータフレーム travelerRatings に読み込む\n",
        "# 　「heading = 0」： データセットの列名の行( category1 や category2 など) を読み込まない\n",
        "# 　「index_col=False」： 一番目の列はインデックスではない。つまり、全ての列をデータ列とする。\n",
        "travelerRatings = pd.read_csv('usr/bin/colab/google_review_ratings.csv', header = 0, names=columnNames, index_col=False)\n",
        "travelerRatings = travelerRatings.set_index('旅行者ID') #旅行者ID列をインデックスとする\n",
        "travelerRatings = travelerRatings.drop('ローカルサービス',axis=1) #この列は崩れたから、削除する。\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"===== 旅行者から旅行先に対する評価 (0.0 ≦ 評点 ≦ 5.0) =====\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(travelerRatings)\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ２： k-means法でクラスタリングを行う\n",
        "#---------------------------------------------------------------------------------------------\n",
        "from sklearn.cluster import KMeans\n",
        "targetData = travelerRatings[targetColumns] #ターゲット列だけを使う\n",
        "\n",
        "#1. エルボー法ででk-means法の最適なクラスター数を推定する。\n",
        "import matplotlib.pyplot as plt\n",
        "#import japanize_matplotlib\n",
        "model = KMeans(n_clusters=i)\n",
        "model.fit(targetData)\n",
        "SSE.append(model.inertia_)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(range(1,10), SSE)\n",
        "plt.xlabel('クラスター数')\n",
        "plt.ylabel('SSE (Sum of Squared Errors)')\n",
        "plt.title('Kの最適値を決めるエルボー法')\n",
        "plt.savefig(\"L5_elbowFig.png\") #この図を使って、\n",
        "#plt.show()                    #　クラスター個数を決める。\n",
        "num_clusters = 3 #クラスター個数を設定する\n",
        "\n",
        "#2. 設定したクラスタ個数で、クラスタリングを行う。\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++開始\n",
        "from sklearn.cluster import KMeans\n",
        "#data = subjectratings.drop('学生',axis=1)\n",
        "k_means = KMeans(n_clusters=2)\n",
        "k_means.fit(targetData) #クラスタリングの計算を実行する\n",
        "labels = k_means.labels_ \t#分類先となったラベルを取得する\n",
        "outputresult = pd.DataFrame(labels, index=travelerRatings.User, columns=['クラスターID'])\n",
        "print(\"------------------\")\n",
        "print(outputresult)\n",
        "print(\"------------------\\n\")\n",
        "\n",
        "\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++終了\n",
        "\n",
        "clusterIDs = KMeans.labels_    # 分類先となったラベルを取得する\n",
        "\n",
        "#Pandasのデータフレーム targetData に分類結果を追加する\n",
        "travelerCluster = targetData.copy()\n",
        "travelerCluster['クラスターID'] = clusterIDs\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(\"============ 旅行者のクラスターへの振り分け結果 ===========\")\n",
        "print(\"-----------------------------------------------------------\")\n",
        "print(travelerCluster)\n",
        "print(\"------------------------\\n\")\n",
        "\n",
        "#ステップ２で得たモデル k_means を適用し、新しいデータの振分けをする\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++開始\n",
        "import numpy as np\n",
        "experData=np.array([[4.87,5,2.82,2.8,2.57,1.74,1.21,0.05,0.64]])\n",
        "experResult=KMeans.predict(experData)\n",
        "print(\"分類の結果\",experResult)\n",
        "\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++終了\n",
        "\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#ステップ３： 各クラスタにラベルをつける\n",
        "#---------------------------------------------------------------------------------------------\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++開始\n",
        "\n",
        "userlabels=list(clusterIDs)\n",
        "claster=([],[],[],[],[],[],[],[],[],[])\n",
        "\n",
        "#+++++++++++++++++                Python命令文を追加してください                 ++++++++++++++終了\n",
        "##\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "SZ2nr7qLXmJ6",
        "outputId": "923e2a37-c774-405b-90c1-49a161b10288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a543acd6ed37>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 　「heading = 0」： データセットの列名の行( category1 や category2 など) を読み込まない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 　「index_col=False」： 一番目の列はインデックスではない。つまり、全ての列をデータ列とする。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtravelerRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'usr/bin/colab/google_review_ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumnNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtravelerRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtravelerRatings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'旅行者ID'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#旅行者ID列をインデックスとする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtravelerRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtravelerRatings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ローカルサービス'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#この列は崩れたから、削除する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'usr/bin/colab/google_review_ratings.csv'"
          ]
        }
      ]
    }
  ]
}
